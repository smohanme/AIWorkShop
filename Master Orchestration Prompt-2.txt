# 📌 ICE-POT Prompts for Insurance Policy FAQ RAG Assistant

Each prompt is self-contained, follows the **ICE-POT** definition, and is crafted to directly generate project deliverables.

## 1 — Project Kickoff: Technical Plan & Milestones

**I (Intent):** Produce a complete technical plan and milestone list.
**C (Context):** Python 3.12.4, FastAPI, Streamlit, `sentence-transformers` (`all-MiniLM-L6-v2`), `faiss-cpu`, pandas, requests, dotenv. No SSL, no admin rights, no venv. FAISS index in `index_store/`. Fallback in-memory if file I/O blocked.
**E (Examples):** Deliverable = backend endpoints (`/index`, `/query?text=...`), Streamlit demo, 3 queries tested.
**P (Persona):** Senior AI Solution Architect.
**O (Output):** Timeline + tasks, effort estimates, acceptance criteria, demo script, risk table.
**T (Tools):** Packages above.

**Prompt:**

> You are a Senior AI Solution Architect. Produce a step-by-step technical plan and milestone schedule (tasks + subtasks) to implement the Insurance Policy FAQ RAG Assistant in 2 hours. Use the provided stack: Python 3.12.4, FastAPI, uvicorn, streamlit, pandas, sentence-transformers (`all-MiniLM-L6-v2`), faiss-cpu, requests, python-dotenv. Use the provided project folder structure. Include: task owner role (backend/dev/frontend/tester), exact commands to run (no sudo, no venv), required files to create, simple test plan with three queries, acceptance criteria (functional + non-functional), and risk mitigations for constraints (no SSL, no admin). Keep plan concise but detailed enough for developers to start immediately.


## 2 — Data Synthesis (Insurance FAQs CSV)

**I:** Generate a synthetic dataset (10–15 FAQ Q&As).
**C:** Output must be CSV-compatible (`id,category,question,answer,source_hint`). Short answers, explicit policy citations. End with disclaimer.
**E:** Include tricky cases: multi-policy, pre-existing damage.
**P:** Insurance Content Writer.
**O:** Ready-to-save CSV + README snippet.
**T:** Domain knowledge only.

**Prompt:**

> You're an Insurance Content Writer. Produce 12 FAQ Q&A pairs for auto+home policy FAQs. Output exactly as CSV rows with headers: `id,category,question,answer,source_hint`. Keep answers factual, short (<=120 words), and end each with "Disclaimer: not legal advice; consult your policy." Include source_hint like `policy-section-4.2`. Provide 2 tricky edge cases: multi-policy and pre-existing damage.


## 3 — Backend: Indexer (indexer.py)

**I:** Generate `indexer.py`.
**C:** Load `data/faqs.csv`, embed Q&A, FAISS index, save index + metadata. Robust error handling.
**E:** CLI usage: `python backend/indexer.py --csv data/faqs.csv --out index_store/`.
**P:** Backend dev.
**O:** Complete `indexer.py` file.
**T:** pandas, faiss, pickle, sentence-transformers, tqdm.

**Prompt:**

> Write `backend/indexer.py`. It must: load `data/faqs.csv`, create embeddings with `SentenceTransformer("all-MiniLM-L6-v2")`, build a FAISS index (use IndexFlatIP or IndexFlatL2 with normalized vectors), save `index_store/faiss.index` and metadata `meta.pkl` containing ids and original texts. Provide CLI usage and robust error handling for file permission issues. Include logging. Output the full Python file.

## 4 — Backend: Retriever (retriever.py)

**I:** Create retriever module.
**C:** Load FAISS + metadata, cosine similarity, return top-k with scores.
**E:** Example: “What is not covered in my collision claim?” → relevant answer.
**P:** Backend dev.
**O:** Full `retriever.py` + small CLI demo.
**T:** faiss, sentence-transformers, pickle, numpy.

**Prompt:**

> Produce `backend/retriever.py` implementing `class Retriever:` with `load_index(path)` and `search(text, k=3)`. It should return list of dicts `{id,question,answer,score,source_hint}`. Include a small CLI demo to query interactively.

## 5 — Backend: LLM Gateway (llm_gateway.py)

**I:** Gateway to Groq API.
**C:** Read API key from `.env`, format RAG prompt with context + citations, handle errors.
**E:** Example call with 2 contexts.
**P:** Backend integrator.
**O:** `backend/llm_gateway.py` + `.env.example`.
**T:** requests, dotenv, json.

**Prompt:**

> Write `backend/llm_gateway.py` that: (1) reads `GROQ_API_KEY` from `.env`, (2) exposes `def ask_with_context(user_question, contexts)` where contexts is a list of `{id,question,answer,source_hint}`, (3) builds a chat body per the Groq API JSON model and returns parsed answer + list of source_hints. Include error handling and sample usage.

## 6 — Backend: Prompt Templates (prompt_templates.py)

**I:** Define safe prompt templates.
**C:** Must enforce citation use and prevent hallucination.
**E:** Example filled template.
**P:** Prompt Engineer.
**O:** Python module with templates + `render()` helper.
**T:** Pure Python.

**Prompt:**

> Create `backend/prompt_templates.py` with these templates: `RAG_ANSWER`, `FOLLOWUP_GENERATOR`, `CLARIFY_IF_NEEDED`. Include comments and usage examples.

## 7 — Backend: FastAPI (main.py)

**I:** API server.
**C:** Endpoints: `/index`, `/search`, `/health`. Use retriever + llm_gateway. CORS enabled.
**E:** Sample `/search?q=claim deadline` → JSON with `answer`, `sources`.
**P:** Backend API engineer.
**O:** Full `main.py`.
**T:** FastAPI, pydantic.

**Prompt:**

> Write `backend/main.py` implementing the endpoints above. Include inline comments and example curl commands.

## 8 — Frontend: Streamlit App

**I:** Demo UI.
**C:** Upload FAQ CSV (optional), query box, answer display with clickable sources, collapsible raw contexts. 3 example queries.
**E:** "Run Demo" button.
**P:** Frontend engineer.
**O:** `frontend/streamlit_app.py` + `frontend/requirements.txt`.
**T:** Streamlit, requests.

**Prompt:**

> Produce `frontend/streamlit_app.py` that calls the FastAPI `/search` endpoint. Include preloaded example queries and a clear UI for showing citations and raw retrieved text. Also include a small README note for how to run the demo locally.

## 9 — Documentation: README.md

**I:** Provide complete README.
**C:** Explain install, indexer, backend, frontend, demo queries. No venv, no SSL, no sudo.
**E:** Include architecture ASCII diagram, curl examples.
**P:** Technical writer.
**O:** README.md.
**T:** Markdown.

**Prompt:**

> Write README.md describing installation (pip install -r requirements.txt), how to run indexer, how to start backend (`uvicorn backend.main:app --reload --port 8000`), run streamlit, environment variables, and demo script (3 queries). Include scoring rubric mapping to evaluation criteria.

## 10 — Security & Ethics Checklist

**I:** Provide concise checklist.
**C:** PII, prompt injection, key handling, SSL absence.
**E:** Add `.gitignore` note.
**P:** Security engineer.
**O:** Checklist text block.
**T:** N/A.

**Prompt:**

> Provide a concise security and ethics checklist for this demo-level RAG assistant, with practical mitigations that match the environment (no SSL, no admin).

## 11 — Demo Script

**I:** Judges script.
**C:** Must use only install, indexer, uvicorn, streamlit commands.
**E:** 3 queries with expected outputs.
**P:** Demo host.
**O:** One-page demo script.
**T:** N/A.

**Prompt:**

> Create a 1-page demo script (commands + UI steps) containing three queries, what to click, and what judge must verify (answer grounded? citation present? latency <X seconds?).

## 12 — Evaluation Rubric

**I:** Provide scoring rubric.
**C:** Criteria: problem addressing, AI concept, data quality, pipeline, code quality, scalability, ethics.
**E:** 0–5 scale, weighted.
**P:** Competition organizer.
**O:** Markdown rubric.
**T:** N/A.

**Prompt:**

> Produce an evaluation rubric in Markdown, with weighted scoring (0–5) for each criterion and example judge comments.

## 13 — Backend Boilerplate (All Files at Once)

**I:** Generate all backend files in one go.
**C:** Provide each file separated by headers. No secrets.
**E:** Include a small `faqs.csv` sample row.
**P:** Full-stack dev.
**O:** Monolithic output with 5 files.
**T:** Python libs above.

**Prompt:**

> Generate the full backend codebase as separate files. Start each file with a comment header `# FILE: backend/<filename>`. Include small inline usage examples and file run commands. Ensure no secrets in code.

## 14 — Frontend UI Spec

**I:** UX polish for Streamlit app.
**C:** Minimalistic, collapsible raw context, accessibility.
**E:** ASCII mockup.
**P:** Product designer.
**O:** Short UI spec.
**T:** Streamlit.

**Prompt:**

> Provide a one-page UI spec for `frontend/streamlit_app.py` that describes layout, components, and interactions for the demo.


## 15 — Agent Workflow (LangChain/AutoGen)

**I:** Spec orchestration workflow.
**C:** If confidence low, ask clarification. Max 2 rounds.
**E:** Example: pre-existing damage.
**P:** Workflow engineer.
**O:** JSON/YAML pseudo workflow.
**T:** LangChain/AutoGen pseudo-language.

**Prompt:**

> Provide a workflow spec for an AutoGen/LangChain agent that: (1) retriever search, (2) check aggregate similarity, (3) if below threshold ask for clarification, else (4) call llm_gateway with contexts and return answer+citations. Include sample messages for each step.

## 16 — Roadmap to Production

**I:** Roadmap beyond demo.
**C:** Milvus migration, cloud LLMs, compliance.
**E:** 3-month plan with KPIs.
**P:** Product manager.
**O:** 1-page roadmap.
**T:** N/A.

**Prompt:**

> Output a 3-month roadmap to take this demo to production, including Milvus migration plan, performance goals, cost considerations, and compliance steps.

## 17 — Auto-Grading Script

**I:** Script to grade demo automatically.
**C:** Runs 3 queries, checks citations + answers.
**E:** Pass/fail messages.
**P:** Tools engineer.
**O:** `scripts/grade_demo.py`.
**T:** requests, pandas.

**Prompt:**

> Create `scripts/grade_demo.py` that executes the three test queries and prints a JSON scorecard.
